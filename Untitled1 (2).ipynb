{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOM4(year,qtr,fips):\n",
    "    host = 'dbeslinx'; DOM3 = []\n",
    "    dnaicsloc = \"/wagerec/current_dominantnaics/parquet/\"\n",
    "    fs = SSHFileSystem(host,username=USER,password=PWD,)\n",
    "    tds = ds.dataset(dnaicsloc, filesystem=fs)\n",
    "    DOM1 = pl.scan_pyarrow_dataset(tds)\n",
    "    DOM2 = (DOM1.filter(pl.col('fips') == fips).filter(pl.col('year') == year)\n",
    "        .filter(pl.col('qtr') == qtr)).filter(pl.col('Dui_level')==True))\n",
    "    Final = DOM2.select(['ui_acct','fips','naics4']).collect().to_pandas()\n",
    "    Final.rename(columns={'ui_acct': 'UI'},inplace=True)\n",
    "    print(f\"{Final.shape[0]} Dominant Naics records found\")\n",
    "    return(Final)\n",
    "\n",
    "def extraction(state, quarter, year):\n",
    "    fips = US.get(state);f=fips.lstrip('0')\n",
    "    domnaics = DOM4(year,quarter,f)\n",
    "    params={'state':fips,'year':year,'qtr':quarter}\n",
    "    WR = pd.read_sql(query,con=conn,params=params)\n",
    "    WR['fips']=WR['STATE_CODE'].astype(str)\n",
    "    WR['RUN'] = WR['RUN'].astype(str)\n",
    "    WR['UI'] = WR['UI'].astype(str)\n",
    "    domnaics['RUN'] = domnaics['RUN'].astype(str)\n",
    "    domnaics['UI'] = domnaics['UI'].astype(str)\n",
    "    B=getbad(BAD,year,quarter)\n",
    "    WR,domnaics=clean(WR,domnaics,B)\n",
    "    WR=dupeID(WR)\n",
    "    A = Cmerge(WR,domnaics)\n",
    "    return A\n",
    "\n",
    "def get_cohort(state, quarter, year):\n",
    "    A = extraction(state, quarter, year)\n",
    "    B = A[A['naics4'] == '6244']\n",
    "    T = B['BLS_ID'].tolist()\n",
    "    file_wage = os.path.join('/moving_sale/B/coh', f'{year}', f'wages_{state}.csv')\n",
    "    B.to_csv(file_wage, index=True)\n",
    "    return T\n",
    "\n",
    "def LOOK(state, year, quarter, cohort):\n",
    "    A = exctraction(state,quarter,year)\n",
    "    matches = A[A['BLS_ID'].isin(cohort)]\n",
    "    file_wage = os.path.join('/moving_sale/B/coh', f'{year}', f'wages_{state}.csv')\n",
    "    matches.to_csv(file_wage, index=True)\n",
    "    print(f\"{matches.shape[0]} {state} matches found in {year} {quarter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "\n",
    "initial_state = 'TX'  \n",
    "initial_year = 2019\n",
    "initial_quarter = 2\n",
    "initial_cohort = get_cohort(initial_state, initial_quarter, initial_year)\n",
    "\n",
    "quarters = []\n",
    "year = 2019\n",
    "quarter = 3\n",
    "while not (year == 2024 and quarter == 2):\n",
    "    quarters.append((year, quarter))\n",
    "    quarter += 1\n",
    "    if quarter > 4:\n",
    "        quarter = 1\n",
    "        year += 1\n",
    "\n",
    "\n",
    "all_states = US.get_states()  \n",
    "\n",
    "def worker(state, year, quarter, cohort):\n",
    "    LOOK(state, year, quarter, cohort)\n",
    "\n",
    "threads = []\n",
    "\n",
    "for state in all_states:\n",
    "    for year, quarter in quarters:\n",
    "        t = threading.Thread(target=worker, args=(state, year, quarter, initial_cohort))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56288732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOOK(state, year, quarter, cohort):\n",
    "    fips = US.get(state).lstrip('0')\n",
    "\n",
    "    # Create cohort filter dynamically for SQL\n",
    "    cohort_filter = \", \".join(f\"'{id_}'\" for id_ in cohort)\n",
    "\n",
    "    # SQL query to find matches\n",
    "    match_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM wage_records\n",
    "    WHERE STATE_CODE = :state\n",
    "      AND YEAR = :year\n",
    "      AND QTR = :qtr\n",
    "      AND BLS_ID IN ({cohort_filter})\n",
    "    \"\"\"\n",
    "\n",
    "    # SQL query to count occurrences of each ID\n",
    "    count_query = f\"\"\"\n",
    "    SELECT BLS_ID, COUNT(*) AS occurrences\n",
    "    FROM wage_records\n",
    "    WHERE STATE_CODE = :state\n",
    "      AND YEAR = :year\n",
    "      AND QTR = :qtr\n",
    "      AND BLS_ID IN ({cohort_filter})\n",
    "    GROUP BY BLS_ID\n",
    "    ORDER BY occurrences DESC\n",
    "    \"\"\"\n",
    "\n",
    "    params = {'state': fips, 'year': year, 'qtr': quarter}\n",
    "\n",
    "    # Execute the queries\n",
    "    matches = pd.read_sql(match_query, con=conn, params=params)\n",
    "    id_counts = pd.read_sql(count_query, con=conn, params=params)\n",
    "\n",
    "    # Save matches and counts to CSV files\n",
    "    match_file = os.path.join('/moving_sale/B/coh', f'{year}', f'wages_{state}_matches.csv')\n",
    "    count_file = os.path.join('/moving_sale/B/coh', f'{year}', f'wages_{state}_id_counts.csv')\n",
    "\n",
    "    matches.to_csv(match_file, index=False)\n",
    "    id_counts.to_csv(count_file, index=False)\n",
    "\n",
    "    # Output confirmation\n",
    "    print(f\"{matches.shape[0]} {state} matches found in {year} {quarter}\")\n",
    "    print(f\"ID occurrences saved to {count_file}\")\n",
    "\n",
    "    \n",
    "    \n",
    "def extraction(state, quarter, year):\n",
    "    fips = US.get(state).lstrip('0')\n",
    "    \n",
    "    # Fetch dominant NAICS IDs (DOM4 function)\n",
    "    domnaics = DOM4(year, quarter, fips)\n",
    "    cohort_ids = domnaics['UI'].tolist()\n",
    "\n",
    "    # Create a cohort filter dynamically for SQL\n",
    "    cohort_filter = \", \".join(f\"'{id_}'\" for id_ in cohort_ids)\n",
    "\n",
    "    # SQL query to retrieve relevant records\n",
    "    match_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM wage_records\n",
    "    WHERE STATE_CODE = :state\n",
    "      AND YEAR = :year\n",
    "      AND QTR = :qtr\n",
    "      AND UI IN ({cohort_filter})\n",
    "    \"\"\"\n",
    "\n",
    "    # SQL query to count occurrences of IDs\n",
    "    count_query = f\"\"\"\n",
    "    SELECT UI, COUNT(*) AS occurrences\n",
    "    FROM wage_records\n",
    "    WHERE STATE_CODE = :state\n",
    "      AND YEAR = :year\n",
    "      AND QTR = :qtr\n",
    "      AND UI IN ({cohort_filter})\n",
    "    GROUP BY UI\n",
    "    ORDER BY occurrences DESC\n",
    "    \"\"\"\n",
    "\n",
    "    params = {'state': fips, 'year': year, 'qtr': quarter}\n",
    "\n",
    "    # Execute the queries\n",
    "    WR = pd.read_sql(match_query, con=conn, params=params)\n",
    "    id_counts = pd.read_sql(count_query, con=conn, params=params)\n",
    "\n",
    "    # Process and clean the data\n",
    "    WR['fips'] = WR['STATE_CODE'].astype(str)\n",
    "    WR['RUN'] = WR['RUN'].astype(str)\n",
    "    WR['UI'] = WR['UI'].astype(str)\n",
    "\n",
    "    domnaics['RUN'] = domnaics['RUN'].astype(str)\n",
    "    domnaics['UI'] = domnaics['UI'].astype(str)\n",
    "\n",
    "    # Apply cleaning and merging\n",
    "    B = getbad(BAD, year, quarter)\n",
    "    WR, domnaics = clean(WR, domnaics, B)\n",
    "    WR = dupeID(WR)\n",
    "    A = Cmerge(WR, domnaics)\n",
    "\n",
    "    # Save ID counts to a CSV file\n",
    "    count_file = os.path.join('/moving_sale/B/coh', f'{year}', f'id_counts_{state}.csv')\n",
    "    id_counts.to_csv(count_file, index=False)\n",
    "\n",
    "    print(f\"ID occurrences saved to {count_file}\")\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "initial_state = 'TX'  \n",
    "initial_year = 2019\n",
    "initial_quarter = 2\n",
    "initial_cohort = get_cohort(initial_state, initial_quarter, initial_year)\n",
    "\n",
    "quarters = []\n",
    "year = 2019\n",
    "quarter = 3\n",
    "while not (year == 2024 and quarter == 2):\n",
    "    quarters.append((year, quarter))\n",
    "    quarter += 1\n",
    "    if quarter > 4:\n",
    "        quarter = 1\n",
    "        year += 1\n",
    "\n",
    "all_states = US.get_states()\n",
    "\n",
    "def worker(state, year, quarter, cohort):\n",
    "    try:\n",
    "        logging.info(f\"Processing {state} {year} Q{quarter}\")\n",
    "        LOOK(state, year, quarter, cohort)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {state} {year} Q{quarter}: {e}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [\n",
    "        executor.submit(worker, state, year, quarter, initial_cohort)\n",
    "        for state in all_states\n",
    "        for year, quarter in quarters\n",
    "    ]\n",
    "\n",
    "    for future in futures:\n",
    "        future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
