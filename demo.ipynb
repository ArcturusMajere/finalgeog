import oracledb
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed

def query_ids_year_qtr_concurrent(ids, start_year, start_qtr, end_year, end_qtr,
                                   dsn, user, password, file_dir,
                                   workers=6, chunk_size=1000):
    """
    Executes concurrent Oracle database queries using connection pooling and
    ThreadPoolExecutor to fetch data for given IDs within a specified year and quarter range.

    Args:
        ids (list): A list of IDs to query for.
        start_year (int): The starting year for the query.
        start_qtr (int): The starting quarter for the query.
        end_year (int): The ending year for the query.
        end_qtr (int): The ending quarter for the query.
        dsn (str): The Oracle Database DSN (Data Source Name).
        user (str): The Oracle Database username.
        password (str): The Oracle Database password.
        file_dir (str): The directory path to save the CSV output.
        workers (int): The number of worker threads to use in the ThreadPoolExecutor.
        chunk_size (int): The number of IDs to process in each query chunk.
    """

    # 1. Create the Oracle Connection Pool
    # Using oracledb.create_pool for the new connection pooling API.
    # It's recommended to set min and max to the same value for fixed-size pools, 
    # {Link: according to the oracledb Documentation https://python-oracledb.readthedocs.io/en/v2.4.1/user_guide/connection_handling.html}. 
    # Using POOL_GETMODE_WAIT ensures threads wait for an available connection if none are free.
    pool = oracledb.create_pool(user=user, password=password, dsn=dsn,
                                min=workers, max=workers, increment=0,
                                homogeneous=True, timeout=60, # Timeout for connections in the pool
                                getmode=oracledb.POOL_GETMODE_WAIT) # Wait for a connection if none are available

    # 2. Define the function to run in each thread (querying a chunk of IDs)
    def run_chunk(id_chunk):
        """
        Queries the database for a chunk of IDs within the specified year and quarter range.
        Acquires a connection from the pool and releases it back upon completion.
        """
        if not id_chunk:
            return []

        # Constructing the placeholder string for IN clause efficiently
        # Example: if id_chunk is [1, 2, 3], ph becomes ":id0,:id1,:id2"
        ph = ",".join([f":id{i}" for i in range(len(id_chunk))])
        params = {f"id{i}": v for i, v in enumerate(id_chunk)}
        params.update({"sy": start_year, "sq": start_qtr,
                       "ey": end_year, "eq": end_qtr})

        # The SQL query to retrieve data
        # Note: Replace 'your_table' with your actual table name
        sql = f"""
        SELECT id,
               fips,
               UI,
               run,
               wage,
               (TO_CHAR(year) || '-' || TO_CHAR(qtr)) AS yr_qtr
        FROM your_table
        WHERE id IN ({ph})
          AND (year > :sy OR (year = :sy AND qtr >= :sq))
          AND (year < :ey OR (year = :ey AND qtr <= :eq))
        """
        
        # Acquire a connection from the pool using a context manager for automatic release
        with pool.acquire() as conn:
            cur = conn.cursor()
            cur.execute(sql, params)
            return cur.fetchall()

    # 3. Divide the IDs into chunks for parallel processing
    chunks = [ids[i:i + chunk_size] for i in range(0, len(ids), chunk_size)]
    out = []

    # 4. Use ThreadPoolExecutor for concurrent execution of chunks
    # max_workers is set to 'workers' to utilize the specified number of threads.
    with ThreadPoolExecutor(max_workers=workers) as ex:
        # Submit each chunk to the executor for processing
        futs = [ex.submit(run_chunk, ch) for ch in chunks]
        # Iterate over completed futures and extend the results list
        for f in as_completed(futs):
            # as_completed() yields futures as they complete, allowing
            # results to be collected as soon as they are ready.
            out.extend(f.result())

    # 5. Close the connection pool when all tasks are complete
    pool.close()

    # 6. Save the results to a CSV file
    with open(file_dir, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["id", "fips", "UI", "run", "wage", "yr_qtr"]) # Header row
        writer.writerows(out)

# Example Usage:
if __name__ == "__main__":
    # Replace these with your actual database connection details and IDs
    sample_ids = list(range(1, 10001)) # Example: 10,000 IDs
    start_year = 2023
    start_qtr = 1
    end_year = 2024
    end_qtr = 2
    dsn = "hostname:port/service_name"  # e.g., "localhost:1521/XE"
    user = "your_username"
    password = "your_password"
    output_file = "output.csv"

    print("Starting concurrent database query...")
    query_ids_year_qtr_concurrent(
        ids=sample_ids,
        start_year=start_year,
        start_qtr=start_qtr,
        end_year=end_year,
        end_qtr=end_qtr,
        dsn=dsn,
        user=user,
        password=password,
        file_dir=output_file,
        workers=8, # Adjust the number of workers based on your system and database capacity
        chunk_size=500 # Adjust chunk size for optimal performance
    )
    print(f"Query complete. Results saved to {output_file}")